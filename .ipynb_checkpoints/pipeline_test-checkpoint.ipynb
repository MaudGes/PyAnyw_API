{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef46eea-74e4-4311-bf8e-dac61a747189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 7999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/_kd6jgl573d2nsgvn5nypvym0000gp/T/ipykernel_28377/180408319.py:56: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom business score after threshold optimization: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maudg1/anaconda3/envs/scoring/lib/python3.9/site-packages/mlflow/types/utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import warnings\n",
    "\n",
    "\"\"\"\n",
    "Pipeline used to easily define the model preprocessing steps. The pipeline is then stored using joblib. \n",
    "The model's signature is also saved to be able to reuse it later while deploying the model.\n",
    "The model itself is then stored along with other files in the 'mlflow_model' folder\n",
    "\n",
    "The selected hyperparaters for xgboost come from previous testing and results stored in mlflow (see mlruns folder)\n",
    "\"\"\"\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    test_df = pd.read_csv('credit_files/application_test.csv')\n",
    "    df = pd.read_csv('credit_files/application_train.csv')\n",
    "    print(\"Test samples: {}\".format(len(test_df)))\n",
    "    \n",
    "    # Merging\n",
    "    df = pd.concat([df,test_df])\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    \n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    \n",
    "    #Only keeping relevant columns\n",
    "    df = df[['EXT_SOURCE_3','EXT_SOURCE_2', 'NAME_EDUCATION_TYPE_Higher education','NAME_INCOME_TYPE_Working',\n",
    "             'NAME_EDUCATION_TYPE_Secondary / secondary special','CODE_GENDER','NAME_CONTRACT_TYPE_Cash loans',\n",
    "             'REGION_RATING_CLIENT', 'FLAG_DOCUMENT_3','TARGET']]\n",
    "\n",
    "    #df = df.dropna(subset=['TARGET','EXT_SOURCE_3','EXT_SOURCE_2','EXT_SOURCE_1'])\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Define predictors (feature columns), while exluding payment rate\n",
    "    predictors = [col for col in df.columns if col not in ['SK_ID_CURR', 'TARGET','PAYMENT_RATE']]\n",
    "\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "#Checking the first part\n",
    "trial_1 = application_test()\n",
    "trial_1\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = trial_1.drop(columns = ['TARGET'])\n",
    "y = trial_1['TARGET']\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaling', StandardScaler()),   # StandardScaler for numerical features\n",
    "    ('model', XGBClassifier(\n",
    "        reg_lambda=2.481367528520412,\n",
    "        max_depth=14,\n",
    "        learning_rate=0.2030510614528276,\n",
    "        n_estimators=173,\n",
    "        colsample_bytree=0.9173502331327696,\n",
    "        reg_alpha=0.02759991820225434,\n",
    "        subsample=0.8330533878126005,\n",
    "        n_jobs=1\n",
    "    ))       # XGBoost model\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply the optimized threshold (0.15) for final classification\n",
    "optimal_threshold = 0.15\n",
    "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Define a custom evaluation metric\n",
    "def custom_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Score personnalisé pour évaluer le coût métier.\n",
    "\n",
    "    Règles de scoring :\n",
    "    - Les vrais positifs (TP) et vrais négatifs (TN) sont récompensés.\n",
    "    - Les faux positifs (FP) pénalisent (bon client prédit mauvais).\n",
    "    - Les faux négatifs (FN) pénalisent plus sévèrement (mauvais client prédit bon).\n",
    "\n",
    "    Ici, nous supposons par exemple que le coût d'un FN est dix fois supérieur à celui d'un FP.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Véritables labels (0 ou 1).\n",
    "        y_pred (array-like): Prédictions (labels binaires).\n",
    "\n",
    "    Returns:\n",
    "        score (float): Score calculé.\n",
    "    \"\"\"\n",
    "    # Si y_pred contient des probabilités ou des vecteurs (multi-classe), on transforme en labels\n",
    "    if y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)  # Classification multi-classes\n",
    "    else:\n",
    "        y_pred = np.round(y_pred).astype(int)  # Classification binaire\n",
    "\n",
    "    # Calcul de la matrice de confusion\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Poids attribués à chaque issue\n",
    "    weight_tp = 1    # Récompense pour TP\n",
    "    weight_tn = 1    # Récompense pour TN\n",
    "    weight_fp = -1   # Pénalité pour FP\n",
    "    weight_fn = -10  # Pénalité pour FN (coût dix fois supérieur)\n",
    "\n",
    "    # Calcul du score personnalisé\n",
    "    score = (weight_tp * tp) + (weight_tn * tn) + (weight_fp * fp) + (weight_fn * fn)\n",
    "    return score\n",
    "\n",
    "# Evaluate the model using the custom metric\n",
    "custom_score = custom_metric(y_test, y_pred)\n",
    "print(f\"Custom business score after threshold optimization: {custom_score}\")\n",
    "\n",
    "'''\n",
    "import joblib\n",
    "joblib.dump(pipeline, '/Users/maudg1/Documents/PythonA_API/pipeline_clients_traintest_4.joblib')\n",
    "\n",
    "\n",
    "from mlflow.models.signature import infer_signature\n",
    "signature = infer_signature(X_train, y_train)\n",
    "\n",
    "mlflow.sklearn.save_model(pipeline, '/Users/maudg1/Documents/PythonA_API/mlflow_model2', signature=signature)\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150562b-5af4-46ce-8366-cc7091cd76fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scoring",
   "language": "python",
   "name": "scoring"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
